{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49202b37-7755-427d-a992-eee997b7d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened dataset saved to synthetic_developer_data_flattened.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('synthetic_developer_data.csv')\n",
    "\n",
    "# Function to flatten nested JSON-like columns\n",
    "def flatten_column(df, column_name):\n",
    "    # Convert string representation of dict to actual dict\n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    # Normalize the column\n",
    "    flattened = pd.json_normalize(df[column_name])\n",
    "    # Rename columns to include the original column name\n",
    "    flattened.columns = [f\"{column_name}_{subcol}\" for subcol in flattened.columns]\n",
    "    return flattened\n",
    "\n",
    "# Flatten all nested columns\n",
    "tab_metrics_flattened = flatten_column(data, 'tabMetrics')\n",
    "copy_paste_metrics_flattened = flatten_column(data, 'copyPasteMetrics')\n",
    "error_metrics_flattened = flatten_column(data, 'errorMetrics')\n",
    "code_metrics_flattened = flatten_column(data, 'codeMetrics')\n",
    "achievements_flattened = flatten_column(data, 'achievements')\n",
    "error_summary_flattened = flatten_column(data, 'errorSummary')\n",
    "\n",
    "# Combine flattened columns with the original dataset\n",
    "data_flattened = pd.concat([\n",
    "    data.drop(columns=['tabMetrics', 'copyPasteMetrics', 'errorMetrics', 'codeMetrics', 'achievements', 'errorSummary']),\n",
    "    tab_metrics_flattened,\n",
    "    copy_paste_metrics_flattened,\n",
    "    error_metrics_flattened,\n",
    "    code_metrics_flattened,\n",
    "    achievements_flattened,\n",
    "    error_summary_flattened\n",
    "], axis=1)\n",
    "\n",
    "# Save the flattened dataset\n",
    "data_flattened.to_csv('synthetic_developer_data_flattened.csv', index=False)\n",
    "print(\"Flattened dataset saved to synthetic_developer_data_flattened.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5ee7437-e16b-40bd-879c-2b800f3192aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         focusScore  currentStreak  longestStreak  sessionDuration  \\\n",
      "count  10000.000000    10000.00000   10000.000000     10000.000000   \n",
      "mean      60.077030        2.50800       5.521300        42.380180   \n",
      "std       16.643439        1.70957       2.871301        10.509056   \n",
      "min       29.600000        0.00000       1.000000        10.000000   \n",
      "25%       42.000000        1.00000       3.000000        35.130000   \n",
      "50%       60.100000        2.00000       6.000000        42.535000   \n",
      "75%       78.100000        4.00000       8.000000        49.615000   \n",
      "max       89.700000        5.00000      10.000000        78.640000   \n",
      "\n",
      "       activeFileDuration      idleTime  typingRhythm      __v  \\\n",
      "count        10000.000000  10000.000000  10000.000000  10000.0   \n",
      "mean            42.380180     59.931710     59.523200      0.0   \n",
      "std             10.509056     25.048929     16.639686      0.0   \n",
      "min             10.000000     10.960000     30.000000      0.0   \n",
      "25%             35.130000     33.397500     41.000000      0.0   \n",
      "50%             42.535000     59.960000     59.000000      0.0   \n",
      "75%             49.615000     86.540000     77.000000      0.0   \n",
      "max             78.640000    108.580000     89.000000      0.0   \n",
      "\n",
      "       tabMetrics_total  tabMetrics_rapid  ...  errorMetrics_problemCount  \\\n",
      "count      10000.000000      10000.000000  ...               10000.000000   \n",
      "mean          10.543000          2.485100  ...                   7.457600   \n",
      "std            5.764796          1.710576  ...                   5.094723   \n",
      "min            1.000000          0.000000  ...                   0.000000   \n",
      "25%            5.000000          1.000000  ...                   4.000000   \n",
      "50%           11.000000          2.000000  ...                   7.000000   \n",
      "75%           16.000000          4.000000  ...                  10.000000   \n",
      "max           20.000000          5.000000  ...                  20.000000   \n",
      "\n",
      "       codeMetrics_linesAdded  codeMetrics_linesDeleted  \\\n",
      "count            10000.000000              10000.000000   \n",
      "mean                98.866700                 24.849600   \n",
      "std                 57.955568                 14.702204   \n",
      "min                  0.000000                  0.000000   \n",
      "25%                 49.000000                 12.000000   \n",
      "50%                 98.000000                 25.000000   \n",
      "75%                149.000000                 37.000000   \n",
      "max                200.000000                 50.000000   \n",
      "\n",
      "       codeMetrics_fileEdits  codeMetrics_codeComplexity  \\\n",
      "count           10000.000000                10000.000000   \n",
      "mean                5.457100                    5.019600   \n",
      "std                 2.880137                    3.165157   \n",
      "min                 1.000000                    0.000000   \n",
      "25%                 3.000000                    2.000000   \n",
      "50%                 5.000000                    5.000000   \n",
      "75%                 8.000000                    8.000000   \n",
      "max                10.000000                   10.000000   \n",
      "\n",
      "       codeMetrics_testCoverage  errorSummary_total  \\\n",
      "count              10000.000000        10000.000000   \n",
      "mean                  50.075100           12.826800   \n",
      "std                   29.058247            7.473579   \n",
      "min                    0.000000            0.000000   \n",
      "25%                   25.000000            7.000000   \n",
      "50%                   50.000000           11.000000   \n",
      "75%                   75.000000           17.000000   \n",
      "max                  100.000000           35.000000   \n",
      "\n",
      "       errorSummary_bySeverity.error  errorSummary_bySeverity.warning  \\\n",
      "count                   10000.000000                     10000.000000   \n",
      "mean                        1.687000                         3.682200   \n",
      "std                         1.425284                         2.625851   \n",
      "min                         0.000000                         0.000000   \n",
      "25%                         1.000000                         2.000000   \n",
      "50%                         1.000000                         3.000000   \n",
      "75%                         2.000000                         5.000000   \n",
      "max                         5.000000                        10.000000   \n",
      "\n",
      "       errorSummary_bySeverity.info  \n",
      "count                  10000.000000  \n",
      "mean                       7.457600  \n",
      "std                        5.094723  \n",
      "min                        0.000000  \n",
      "25%                        4.000000  \n",
      "50%                        7.000000  \n",
      "75%                       10.000000  \n",
      "max                       20.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n",
      "                        _id                   timestamp  userId  focusScore  \\\n",
      "0  678b4b6b45ee875d7437c6a0  2025-01-19T00:01:32.914173  user_2        58.0   \n",
      "1  678b4b6b45ee875d7437c6a1  2025-01-19T00:07:32.914173  user_3        42.1   \n",
      "2  678b4b6b45ee875d7437c6a2  2025-01-19T00:16:32.914173  user_3        80.2   \n",
      "3  678b4b6b45ee875d7437c6a3  2025-01-19T00:20:32.914173  user_2        58.9   \n",
      "4  678b4b6b45ee875d7437c6a4  2025-01-19T00:29:32.914173  user_3        78.9   \n",
      "\n",
      "   currentStreak  longestStreak  sessionDuration  activeFileDuration  \\\n",
      "0              0              2            45.48               45.48   \n",
      "1              1              2            47.10               47.10   \n",
      "2              5              6            46.59               46.59   \n",
      "3              1              2            16.50               16.50   \n",
      "4              2              4            36.66               36.66   \n",
      "\n",
      "   idleTime  typingRhythm  ... codeMetrics_linesDeleted  \\\n",
      "0     58.41            60  ...                       44   \n",
      "1     85.60            35  ...                       49   \n",
      "2     30.72            74  ...                       49   \n",
      "3     64.39            57  ...                       18   \n",
      "4     33.61            75  ...                       33   \n",
      "\n",
      "   codeMetrics_fileEdits codeMetrics_codeComplexity  codeMetrics_testCoverage  \\\n",
      "0                      8                          1                        21   \n",
      "1                      4                          1                        44   \n",
      "2                      5                          6                        28   \n",
      "3                      5                          5                        26   \n",
      "4                      4                          8                        54   \n",
      "\n",
      "   achievements_0  errorSummary_total  errorSummary_recent  \\\n",
      "0             NaN                  15                   []   \n",
      "1             NaN                  25                   []   \n",
      "2             NaN                   6                   []   \n",
      "3             NaN                   8                   []   \n",
      "4             NaN                   8                   []   \n",
      "\n",
      "   errorSummary_bySeverity.error  errorSummary_bySeverity.warning  \\\n",
      "0                              1                                5   \n",
      "1                              1                                8   \n",
      "2                              0                                4   \n",
      "3                              1                                5   \n",
      "4                              2                                4   \n",
      "\n",
      "   errorSummary_bySeverity.info  \n",
      "0                             9  \n",
      "1                            16  \n",
      "2                             2  \n",
      "3                             2  \n",
      "4                             2  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load flattened dataset\n",
    "data_flattened = pd.read_csv('synthetic_developer_data_flattened.csv')\n",
    "print(data_flattened.describe())\n",
    "print(data_flattened.head())\n",
    "\n",
    "# Select features and targets\n",
    "features = data_flattened.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "target_reg = 'focusScore'\n",
    "target_clf = 'productivityStatus'\n",
    "\n",
    "# Prepare data for regression\n",
    "X = data_flattened[features]\n",
    "y_reg = data_flattened[target_reg]\n",
    "\n",
    "# Prepare data for classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_clf = label_encoder.fit_transform(data_flattened[target_clf])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_clf, y_test_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1dcdfdd2-279f-4525-9f8d-3791e8dde513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 23:45:07,964 - INFO - Evaluating Regression Models (focusScore):\n",
      "2025-01-18 23:45:08,115 - INFO - Linear Regression: CV MSE = 0.0000, Test MSE = 0.0000\n",
      "2025-01-18 23:45:32,062 - INFO - Random Forest Regression: CV MSE = 0.0040, Test MSE = 0.0013\n",
      "2025-01-18 23:45:41,605 - INFO - Gradient Boosting Regression: CV MSE = 0.0155, Test MSE = 0.0140\n",
      "2025-01-18 23:45:41,610 - INFO - \n",
      "Evaluating Classification Models (productivityStatus):\n",
      "2025-01-18 23:45:42,317 - INFO - Logistic Regression: CV Accuracy = 0.9863, Test Accuracy = 0.9895\n",
      "2025-01-18 23:45:42,318 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Distracted ðŸ˜•       0.97      0.99      0.98       338\n",
      "   Focused ðŸŽ¯       0.99      0.99      0.99       682\n",
      "   In Flow ðŸŒŠ       0.99      0.99      0.99       980\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n",
      "2025-01-18 23:45:45,918 - INFO - Random Forest Classification: CV Accuracy = 1.0000, Test Accuracy = 1.0000\n",
      "2025-01-18 23:45:45,919 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Distracted ðŸ˜•       1.00      1.00      1.00       338\n",
      "   Focused ðŸŽ¯       1.00      1.00      1.00       682\n",
      "   In Flow ðŸŒŠ       1.00      1.00      1.00       980\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "2025-01-18 23:46:03,730 - INFO - Gradient Boosting Classification: CV Accuracy = 1.0000, Test Accuracy = 1.0000\n",
      "2025-01-18 23:46:03,731 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Distracted ðŸ˜•       1.00      1.00      1.00       338\n",
      "   Focused ðŸŽ¯       1.00      1.00      1.00       682\n",
      "   In Flow ðŸŒŠ       1.00      1.00      1.00       980\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "2025-01-18 23:46:03,783 - INFO - All models and preprocessing objects have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load flattened dataset\n",
    "data_flattened = pd.read_csv('synthetic_developer_data_flattened.csv')\n",
    "\n",
    "# Select features and targets\n",
    "features = data_flattened.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "target_reg = 'focusScore'\n",
    "target_clf = 'productivityStatus'\n",
    "\n",
    "# Prepare data for regression\n",
    "X = data_flattened[features]\n",
    "y_reg = data_flattened[target_reg]\n",
    "\n",
    "# Prepare data for classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_clf = label_encoder.fit_transform(data_flattened[target_clf])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_clf, y_test_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Regression Models (Predicting focusScore)\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regression\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting Regression\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Classification Models (Predicting productivityStatus)\n",
    "models_clf = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest Classification\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting Classification\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate Regression Models\n",
    "logging.info(\"Evaluating Regression Models (focusScore):\")\n",
    "reg_results = {}\n",
    "for name, model in models_reg.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train_reg, scoring='neg_mean_squared_error', cv=5)\n",
    "    cv_mse = -np.mean(cv_scores)\n",
    "    \n",
    "    # Train and evaluate on test set\n",
    "    model.fit(X_train, y_train_reg)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    reg_results[name] = {\n",
    "        \"CV MSE\": cv_mse,\n",
    "        \"Test MSE\": mse\n",
    "    }\n",
    "    \n",
    "    # Log results\n",
    "    logging.info(f\"{name}: CV MSE = {cv_mse:.4f}, Test MSE = {mse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, f'{name.lower().replace(\" \", \"_\")}_reg_model.pkl')\n",
    "\n",
    "# Evaluate Classification Models\n",
    "logging.info(\"\\nEvaluating Classification Models (productivityStatus):\")\n",
    "clf_results = {}\n",
    "for name, model in models_clf.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train_clf, scoring='accuracy', cv=5)\n",
    "    cv_accuracy = np.mean(cv_scores)\n",
    "    \n",
    "    # Train and evaluate on test set\n",
    "    model.fit(X_train, y_train_clf)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_clf, y_pred)\n",
    "    clf_report = classification_report(y_test_clf, y_pred, target_names=label_encoder.classes_)\n",
    "    \n",
    "    # Save results\n",
    "    clf_results[name] = {\n",
    "        \"CV Accuracy\": cv_accuracy,\n",
    "        \"Test Accuracy\": accuracy,\n",
    "        \"Classification Report\": clf_report\n",
    "    }\n",
    "    \n",
    "    # Log results\n",
    "    logging.info(f\"{name}: CV Accuracy = {cv_accuracy:.4f}, Test Accuracy = {accuracy:.4f}\")\n",
    "    logging.info(f\"Classification Report:\\n{clf_report}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, f'{name.lower().replace(\" \", \"_\")}_clf_model.pkl')\n",
    "\n",
    "# Save evaluation metrics to a file\n",
    "with open('model_evaluation_metrics.txt', 'w') as f:\n",
    "    f.write(\"Regression Models:\\n\")\n",
    "    for name, metrics in reg_results.items():\n",
    "        f.write(f\"{name}:\\n  CV MSE = {metrics['CV MSE']:.4f}\\n  Test MSE = {metrics['Test MSE']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"\\nClassification Models:\\n\")\n",
    "    for name, metrics in clf_results.items():\n",
    "        f.write(f\"{name}:\\n  CV Accuracy = {metrics['CV Accuracy']:.4f}\\n  Test Accuracy = {metrics['Test Accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Classification Report:\\n{metrics['Classification Report']}\\n\\n\")\n",
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "logging.info(\"All models and preprocessing objects have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8ad6dfb-b629-4ac2-ac82-70019387b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_289907/3369723892.py:19: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['errorSummary_bySeverity_error', 'errorSummary_bySeverity_warning',\\n       'errorSummary_bySeverity_info'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m session_model\u001b[38;5;241m.\u001b[39mfit(X_session, y_session)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Error Severity Analysis Model\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m X_severity \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merrorSummary_bySeverity_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merrorSummary_bySeverity_warning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merrorSummary_bySeverity_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    117\u001b[0m y_severity \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfocusScore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Train a clustering model\u001b[39;00m\n",
      "File \u001b[0;32m~/FlowSense/Analytics/venv/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/FlowSense/Analytics/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/FlowSense/Analytics/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['errorSummary_bySeverity_error', 'errorSummary_bySeverity_warning',\\n       'errorSummary_bySeverity_info'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Load flattened dataset\n",
    "df = pd.read_csv('synthetic_developer_data_flattened.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Handle missing values\n",
    "df['achievements_0'].fillna('No Achievement', inplace=True)\n",
    "\n",
    "# Normalize/Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = [\n",
    "    'currentStreak', 'longestStreak', 'sessionDuration', 'activeFileDuration', \n",
    "    'idleTime', 'typingRhythm', 'tabMetrics_total', 'tabMetrics_rapid', \n",
    "    'codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', \n",
    "    'codeMetrics_codeComplexity', 'codeMetrics_testCoverage'\n",
    "]\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "df['userId'] = df['userId'].astype('category').cat.codes\n",
    "\n",
    "# Feature and Target Selection\n",
    "# Focus Score Prediction\n",
    "X_focus = df[['currentStreak', 'longestStreak', 'sessionDuration', 'activeFileDuration', 'idleTime', 'typingRhythm']]\n",
    "y_focus = df['focusScore']\n",
    "\n",
    "# Code Quality Prediction (assuming code quality is binary: 1 = High, 0 = Low)\n",
    "df['codeQuality'] = np.where(df['errorMetrics_problemCount'] <= 5, 1, 0)  # Example threshold\n",
    "X_quality = df[['codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', 'codeMetrics_codeComplexity', 'codeMetrics_testCoverage']]\n",
    "y_quality = df['codeQuality']\n",
    "\n",
    "# Error Prediction\n",
    "X_error = df[['codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', 'codeMetrics_codeComplexity', 'codeMetrics_testCoverage']]\n",
    "y_error = df['errorMetrics_problemCount']\n",
    "\n",
    "# Productivity Clustering\n",
    "X_productivity = df[['sessionDuration', 'activeFileDuration', 'idleTime', 'typingRhythm']]\n",
    "\n",
    "# Split Data\n",
    "X_focus_train, X_focus_test, y_focus_train, y_focus_test = train_test_split(X_focus, y_focus, test_size=0.2, random_state=42)\n",
    "X_quality_train, X_quality_test, y_quality_train, y_quality_test = train_test_split(X_quality, y_quality, test_size=0.2, random_state=42)\n",
    "X_error_train, X_error_test, y_error_train, y_error_test = train_test_split(X_error, y_error, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Models\n",
    "# Focus Score Prediction Model\n",
    "focus_model = RandomForestRegressor(random_state=42)\n",
    "focus_model.fit(X_focus_train, y_focus_train)\n",
    "\n",
    "# Code Quality Prediction Model\n",
    "quality_model = RandomForestClassifier(random_state=42)\n",
    "quality_model.fit(X_quality_train, y_quality_train)\n",
    "\n",
    "# Error Prediction Model\n",
    "error_model = RandomForestRegressor(random_state=42)\n",
    "error_model.fit(X_error_train, y_error_train)\n",
    "\n",
    "# Productivity Clustering (K-Means)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # 3 clusters: High, Medium, Low productivity\n",
    "df['productivityCluster'] = kmeans.fit_predict(X_productivity)\n",
    "\n",
    "# Typing Rhythm Optimization Model\n",
    "X_typing = df[['typingRhythm', 'sessionDuration', 'activeFileDuration', 'idleTime']]\n",
    "y_typing = df['focusScore']\n",
    "\n",
    "# Train a regression model\n",
    "typing_model = LinearRegression()\n",
    "typing_model.fit(X_typing, y_typing)\n",
    "\n",
    "# Tab Switching Impact Model\n",
    "X_tab = df[['tabMetrics_total', 'tabMetrics_rapid']]\n",
    "y_tab = df['focusScore']\n",
    "\n",
    "# Train a regression model\n",
    "tab_model = LinearRegression()\n",
    "tab_model.fit(X_tab, y_tab)\n",
    "\n",
    "# Code Complexity vs. Error Rate Model\n",
    "X_complexity = df[['codeMetrics_codeComplexity', 'codeMetrics_testCoverage']]\n",
    "y_complexity = df['errorMetrics_problemCount']\n",
    "\n",
    "# Train a regression model\n",
    "complexity_model = LinearRegression()\n",
    "complexity_model.fit(X_complexity, y_complexity)\n",
    "\n",
    "# Achievement Impact Model\n",
    "# Encode achievements as binary features\n",
    "df['hasAchievement'] = df['achievements_0'].apply(lambda x: 1 if x != 'No Achievement' else 0)\n",
    "X_achievement = df[['hasAchievement', 'currentStreak', 'longestStreak']]\n",
    "y_achievement = df['focusScore']\n",
    "\n",
    "# Train a regression model\n",
    "achievement_model = LinearRegression()\n",
    "achievement_model.fit(X_achievement, y_achievement)\n",
    "\n",
    "# Session Duration vs. Productivity Model\n",
    "X_session = df[['sessionDuration', 'activeFileDuration', 'idleTime']]\n",
    "y_session = df['focusScore']\n",
    "\n",
    "# Train a regression model\n",
    "session_model = LinearRegression()\n",
    "session_model.fit(X_session, y_session)\n",
    "\n",
    "# Error Severity Analysis Model\n",
    "X_severity = df[['errorSummary_bySeverity_error', 'errorSummary_bySeverity_warning', 'errorSummary_bySeverity_info']]\n",
    "y_severity = df['focusScore']\n",
    "\n",
    "# Train a clustering model\n",
    "severity_model = KMeans(n_clusters=3, random_state=42)  # 3 clusters: High, Medium, Low severity\n",
    "df['severityCluster'] = severity_model.fit_predict(X_severity)\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(focus_model, 'focus_score_model.pkl')\n",
    "joblib.dump(quality_model, 'code_quality_model.pkl')\n",
    "joblib.dump(error_model, 'error_prediction_model.pkl')\n",
    "joblib.dump(kmeans, 'productivity_clustering_model.pkl')\n",
    "joblib.dump(typing_model, 'typing_rhythm_model.pkl')\n",
    "joblib.dump(tab_model, 'tab_switching_model.pkl')\n",
    "joblib.dump(complexity_model, 'code_complexity_model.pkl')\n",
    "joblib.dump(achievement_model, 'achievement_impact_model.pkl')\n",
    "joblib.dump(session_model, 'session_duration_model.pkl')\n",
    "joblib.dump(severity_model, 'error_severity_model.pkl')\n",
    "\n",
    "# Visualizations\n",
    "# Focus Score vs Session Duration\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['sessionDuration'], y=df['focusScore'], hue=df['productivityCluster'], palette='viridis')\n",
    "plt.title('Focus Score vs Session Duration')\n",
    "plt.xlabel('Session Duration')\n",
    "plt.ylabel('Focus Score')\n",
    "plt.show()\n",
    "\n",
    "# Code Quality Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=df['codeQuality'], palette='Set2')\n",
    "plt.title('Code Quality Distribution')\n",
    "plt.xlabel('Code Quality (1 = High, 0 = Low)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Error Count vs Code Complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['codeMetrics_codeComplexity'], y=df['errorMetrics_problemCount'], hue=df['codeQuality'], palette='coolwarm')\n",
    "plt.title('Error Count vs Code Complexity')\n",
    "plt.xlabel('Code Complexity')\n",
    "plt.ylabel('Error Count')\n",
    "plt.show()\n",
    "\n",
    "# Productivity Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df['productivityCluster'], y=df['sessionDuration'], palette='Set3')\n",
    "plt.title('Productivity Clusters by Session Duration')\n",
    "plt.xlabel('Productivity Cluster')\n",
    "plt.ylabel('Session Duration')\n",
    "plt.show()\n",
    "\n",
    "# Typing Rhythm vs Focus Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['typingRhythm'], y=df['focusScore'], hue=df['productivityCluster'], palette='viridis')\n",
    "plt.title('Typing Rhythm vs Focus Score')\n",
    "plt.xlabel('Typing Rhythm')\n",
    "plt.ylabel('Focus Score')\n",
    "plt.show()\n",
    "\n",
    "# Tab Switching vs Focus Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['tabMetrics_total'], y=df['focusScore'], hue=df['productivityCluster'], palette='coolwarm')\n",
    "plt.title('Tab Switching vs Focus Score')\n",
    "plt.xlabel('Total Tab Switches')\n",
    "plt.ylabel('Focus Score')\n",
    "plt.show()\n",
    "\n",
    "# Achievement Impact on Focus Score\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df['hasAchievement'], y=df['focusScore'], palette='Set3')\n",
    "plt.title('Achievement Impact on Focus Score')\n",
    "plt.xlabel('Has Achievement (1 = Yes, 0 = No)')\n",
    "plt.ylabel('Focus Score')\n",
    "plt.show()\n",
    "\n",
    "# Error Severity Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=df['severityCluster'], palette='Set1')\n",
    "plt.title('Error Severity Clusters')\n",
    "plt.xlabel('Severity Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1976e91b-7e79-428c-a35e-2cf0e1ef36ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Actionable Insights ===\n",
      "\n",
      "1. Focus Score: 56.86\n",
      "   - Your focus score is good. Keep maintaining your workflow!\n",
      "\n",
      "2. Code Quality: Low\n",
      "   - Your code quality needs improvement. Focus on reducing code complexity and increasing test coverage.\n",
      "\n",
      "3. Predicted Error Count: 7.05\n",
      "\n",
      "4. Productivity Cluster: Medium\n",
      "\n",
      "5. Typing Rhythm Impact: 60.16\n",
      "\n",
      "6. Tab Switching Impact: 60.16\n",
      "\n",
      "7. Code Complexity Impact: 7.55\n",
      "   - High code complexity detected. Consider refactoring your code to improve maintainability.\n",
      "\n",
      "9. Session Duration Impact: 60.16\n",
      "\n",
      "10. Error Severity Cluster: Low\n",
      "\n",
      "=== End of Insights ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load all saved models\n",
    "focus_model = joblib.load('focus_score_model.pkl')\n",
    "quality_model = joblib.load('code_quality_model.pkl')\n",
    "error_model = joblib.load('error_prediction_model.pkl')\n",
    "productivity_model = joblib.load('productivity_clustering_model.pkl')\n",
    "typing_model = joblib.load('typing_rhythm_model.pkl')\n",
    "tab_model = joblib.load('tab_switching_model.pkl')\n",
    "complexity_model = joblib.load('code_complexity_model.pkl')\n",
    "session_model = joblib.load('session_duration_model.pkl')\n",
    "severity_model = joblib.load('error_severity_model.pkl')\n",
    "\n",
    "# Example new data (replace with actual data)\n",
    "new_data = pd.DataFrame({\n",
    "    'currentStreak': [3],\n",
    "    'longestStreak': [7],\n",
    "    'sessionDuration': [40],\n",
    "    'activeFileDuration': [700],\n",
    "    'idleTime': [300],\n",
    "    'typingRhythm': [60],\n",
    "    'tabMetrics.total': [10],\n",
    "    'tabMetrics.rapid': [2],\n",
    "    'codeMetrics.linesAdded': [100],\n",
    "    'codeMetrics.linesDeleted': [25],\n",
    "    'codeMetrics.fileEdits': [5],\n",
    "    'codeMetrics.codeComplexity': [6],\n",
    "    'codeMetrics.testCoverage': [50],\n",
    "    'errorMetrics.problemCount': [8],\n",
    "    'errorSummary.bySeverity.error': [2],\n",
    "    'errorSummary.bySeverity.warning': [4],\n",
    "    'errorSummary.bySeverity.info': [6],\n",
    "    'achievements.0': ['No Achievement']\n",
    "})\n",
    "\n",
    "# Replace dots with underscores in column names\n",
    "new_data.columns = [col.replace('.', '_') for col in new_data.columns]\n",
    "\n",
    "# Preprocess new data (same as training preprocessing)\n",
    "numerical_features = [\n",
    "    'currentStreak', 'longestStreak', 'sessionDuration', 'activeFileDuration', \n",
    "    'idleTime', 'typingRhythm', 'tabMetrics_total', 'tabMetrics_rapid', \n",
    "    'codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', \n",
    "    'codeMetrics_codeComplexity', 'codeMetrics_testCoverage'\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "new_data[numerical_features] = scaler.fit_transform(new_data[numerical_features])\n",
    "\n",
    "# Encode categorical variables\n",
    "new_data['userId'] = 0  # Example user ID\n",
    "new_data['hasAchievement'] = new_data['achievements_0'].apply(lambda x: 1 if x != 'No Achievement' else 0)\n",
    "\n",
    "# Make predictions using all models\n",
    "focus_score = focus_model.predict(new_data[['currentStreak', 'longestStreak', 'sessionDuration', 'activeFileDuration', 'idleTime', 'typingRhythm']])\n",
    "code_quality = quality_model.predict(new_data[['codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', 'codeMetrics_codeComplexity', 'codeMetrics_testCoverage']])\n",
    "error_count = error_model.predict(new_data[['codeMetrics_linesAdded', 'codeMetrics_linesDeleted', 'codeMetrics_fileEdits', 'codeMetrics_codeComplexity', 'codeMetrics_testCoverage']])\n",
    "productivity_cluster = productivity_model.predict(new_data[['sessionDuration', 'activeFileDuration', 'idleTime', 'typingRhythm']])\n",
    "typing_rhythm_impact = typing_model.predict(new_data[['typingRhythm', 'sessionDuration', 'activeFileDuration', 'idleTime']])\n",
    "tab_switching_impact = tab_model.predict(new_data[['tabMetrics_total', 'tabMetrics_rapid']])\n",
    "complexity_impact = complexity_model.predict(new_data[['codeMetrics_codeComplexity', 'codeMetrics_testCoverage']])\n",
    "session_impact = session_model.predict(new_data[['sessionDuration', 'activeFileDuration', 'idleTime']])\n",
    "severity_cluster = severity_model.predict(new_data[['errorSummary_bySeverity_error', 'errorSummary_bySeverity_warning', 'errorSummary_bySeverity_info']])\n",
    "\n",
    "# Generate Actionable Insights\n",
    "print(\"\\n=== Actionable Insights ===\")\n",
    "\n",
    "# Focus Score Insights\n",
    "print(f\"\\n1. Focus Score: {focus_score[0]:.2f}\")\n",
    "if focus_score[0] < 50:\n",
    "    print(\"   - Your focus score is low. Try reducing idle time and avoiding rapid tab switching.\")\n",
    "else:\n",
    "    print(\"   - Your focus score is good. Keep maintaining your workflow!\")\n",
    "\n",
    "# Code Quality Insights\n",
    "print(f\"\\n2. Code Quality: {'High' if code_quality[0] == 1 else 'Low'}\")\n",
    "if code_quality[0] == 0:\n",
    "    print(\"   - Your code quality needs improvement. Focus on reducing code complexity and increasing test coverage.\")\n",
    "\n",
    "# Error Count Insights\n",
    "print(f\"\\n3. Predicted Error Count: {error_count[0]:.2f}\")\n",
    "if error_count[0] > 10:\n",
    "    print(\"   - High error count predicted. Review your code changes carefully and write more unit tests.\")\n",
    "\n",
    "# Productivity Insights\n",
    "productivity_labels = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "print(f\"\\n4. Productivity Cluster: {productivity_labels[productivity_cluster[0]]}\")\n",
    "if productivity_cluster[0] == 0:\n",
    "    print(\"   - Your productivity is low. Try scheduling shorter, more focused work sessions.\")\n",
    "\n",
    "# Typing Rhythm Insights\n",
    "print(f\"\\n5. Typing Rhythm Impact: {typing_rhythm_impact[0]:.2f}\")\n",
    "if typing_rhythm_impact[0] < 50:\n",
    "    print(\"   - Your typing rhythm is affecting your focus. Try maintaining a consistent typing speed.\")\n",
    "\n",
    "# Tab Switching Insights\n",
    "print(f\"\\n6. Tab Switching Impact: {tab_switching_impact[0]:.2f}\")\n",
    "if tab_switching_impact[0] < 50:\n",
    "    print(\"   - Excessive tab switching is reducing your focus. Avoid unnecessary tab switches.\")\n",
    "\n",
    "# Code Complexity Insights\n",
    "print(f\"\\n7. Code Complexity Impact: {complexity_impact[0]:.2f}\")\n",
    "if complexity_impact[0] > 7:\n",
    "    print(\"   - High code complexity detected. Consider refactoring your code to improve maintainability.\")\n",
    "\n",
    "# Session Duration Insights\n",
    "print(f\"\\n9. Session Duration Impact: {session_impact[0]:.2f}\")\n",
    "if session_impact[0] < 50:\n",
    "    print(\"   - Your session duration is too short. Try longer, uninterrupted work sessions.\")\n",
    "\n",
    "# Error Severity Insights\n",
    "severity_labels = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "print(f\"\\n10. Error Severity Cluster: {severity_labels[severity_cluster[0]]}\")\n",
    "if severity_cluster[0] == 2:\n",
    "    print(\"   - High-severity errors detected. Prioritize fixing these errors to improve code quality.\")\n",
    "\n",
    "print(\"\\n=== End of Insights ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2634f9b3-ecc0-414b-a6b0-9b51f3c129de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10738.72s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catppuccin in /home/anirudh/FlowSense/Analytics/venv/lib/python3.13/site-packages (2.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install catppuccin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_developer_data_flattened.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24011514-d864-497e-902c-44cf41fbc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Set Catppuccin Mocha colors for Plotly\n",
    "mocha_colors_plotly = {\n",
    "    \"base\": \"#1e1e2e\",\n",
    "    \"text\": \"#cdd6f4\",\n",
    "    \"blue\": \"#89b4fa\",\n",
    "    \"pink\": \"#f5c2e7\",\n",
    "    \"overlay0\": \"#6c7086\"\n",
    "}\n",
    "\n",
    "# 1. Focus Score Over Time (Filtered) - Plotly\n",
    "fig = px.line(df_filtered_focus, x='timestamp', y='focusScore', title='Focus Score Over Time (Filtered)')\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=mocha_colors_plotly[\"base\"],\n",
    "    paper_bgcolor=mocha_colors_plotly[\"base\"],\n",
    "    font_color=mocha_colors_plotly[\"text\"],\n",
    "    xaxis=dict(gridcolor=mocha_colors_plotly[\"overlay0\"]),\n",
    "    yaxis=dict(gridcolor=mocha_colors_plotly[\"overlay0\"])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 2. Typing Rhythm Distribution (Filtered) - Plotly\n",
    "fig = px.histogram(df_filtered_typing, x='typingRhythm', title='Typing Rhythm Distribution (Filtered)')\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=mocha_colors_plotly[\"base\"],\n",
    "    paper_bgcolor=mocha_colors_plotly[\"base\"],\n",
    "    font_color=mocha_colors_plotly[\"text\"],\n",
    "    xaxis=dict(gridcolor=mocha_colors_plotly[\"overlay0\"]),\n",
    "    yaxis=dict(gridcolor=mocha_colors_plotly[\"overlay0\"])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5025da25-0e77-4449-8a45-72ba33205a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"  # Render plots in the browser\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'codeMetrics_linesAdded': [100, 150, 200],\n",
    "    'codeMetrics_linesDeleted': [25, 30, 35],\n",
    "    'codeMetrics_fileEdits': [5, 10, 15],\n",
    "    'codeMetrics_codeComplexity': [6, 7, 8],\n",
    "    'codeMetrics_testCoverage': [50, 60, 70]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate mean values\n",
    "code_metrics = df.mean()\n",
    "\n",
    "# Catppuccin Mocha color palette\n",
    "catppuccin_mocha = {\n",
    "    \"base\": \"#1e1e2e\",\n",
    "    \"text\": \"#cdd6f4\",\n",
    "    \"purple\": \"#C3B1E1\",\n",
    "    \"pink\": \"#f5c2e7\",\n",
    "    \"overlay0\": \"#6c7086\"\n",
    "}\n",
    "\n",
    "# Create the radar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=code_metrics.values,\n",
    "    theta=code_metrics.index,\n",
    "    fill='toself',\n",
    "    name='Code Metrics',\n",
    "    fillcolor=catppuccin_mocha['purple'],  # Fill color\n",
    "    line=dict(color=catppuccin_mocha['pink'])  # Line color (changed to 'pink')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, max(code_metrics.values)],\n",
    "            color=catppuccin_mocha['text']  # Radial axis color\n",
    "        ),\n",
    "        angularaxis=dict(\n",
    "            color=catppuccin_mocha['text']  # Angular axis color\n",
    "        ),\n",
    "        bgcolor=catppuccin_mocha['base']  # Background color\n",
    "    ),\n",
    "    title='Code Quality Metrics (Radar Chart)',\n",
    "    title_font=dict(color=catppuccin_mocha['text']),  # Title color\n",
    "    font=dict(color=catppuccin_mocha['text']),  # General text color\n",
    "    paper_bgcolor=catppuccin_mocha['base'],  # Paper background color\n",
    "    plot_bgcolor=catppuccin_mocha['base'],  # Plot background color\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        font=dict(color=catppuccin_mocha['text'])  # Legend text color\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "424b57e9-b2f1-4112-aea4-30c6df56d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 23:59:56,135 - INFO - Evaluating Regression Models (focusScore):\n",
      "2025-01-18 23:59:56,284 - INFO - Linear Regression: CV MSE = 13.8661, Test MSE = 13.5931\n",
      "2025-01-19 00:00:12,699 - INFO - Random Forest Regression: CV MSE = 9.9900, Test MSE = 10.0318\n",
      "2025-01-19 00:00:16,910 - INFO - Gradient Boosting Regression: CV MSE = 9.3556, Test MSE = 9.3407\n",
      "2025-01-19 00:00:16,952 - INFO - All models and preprocessing objects have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load flattened dataset\n",
    "data_flattened = pd.read_csv('synthetic_developer_data_flattened.csv')\n",
    "\n",
    "# Select features and target\n",
    "features = ['currentStreak', 'longestStreak', 'sessionDuration', 'activeFileDuration', 'idleTime', 'typingRhythm']\n",
    "target_reg = 'focusScore'\n",
    "\n",
    "# Prepare data for regression\n",
    "X = data_flattened[features]\n",
    "y_reg = data_flattened[target_reg]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Regression Models (Predicting focusScore)\n",
    "models_reg = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regression\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting Regression\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate Regression Models\n",
    "logging.info(\"Evaluating Regression Models (focusScore):\")\n",
    "reg_results = {}\n",
    "for name, model in models_reg.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train_reg, scoring='neg_mean_squared_error', cv=5)\n",
    "    cv_mse = -np.mean(cv_scores)\n",
    "    \n",
    "    # Train and evaluate on test set\n",
    "    model.fit(X_train, y_train_reg)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    reg_results[name] = {\n",
    "        \"CV MSE\": cv_mse,\n",
    "        \"Test MSE\": mse\n",
    "    }\n",
    "    \n",
    "    # Log results\n",
    "    logging.info(f\"{name}: CV MSE = {cv_mse:.4f}, Test MSE = {mse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, f'{name.lower().replace(\" \", \"_\")}_reg_model.pkl')\n",
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "logging.info(\"All models and preprocessing objects have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048946e-18b2-4c45-b332-c7bf2e2be930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
